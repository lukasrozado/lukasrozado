<h1 align="center">ðŸ‘‹ Hi, I'm <strong>Lukas Rozado</strong></h1>
<p align="center">
  <strong>Data Analyst & Data Engineering Enthusiast</strong> â€” I build reliable data pipelines, analytical systems, and long-term predictive modeling projects.
</p>

<p align="center">
  <img alt="stack" src="https://img.shields.io/badge/Python-3.10+-blue" />
  <img alt="stack" src="https://img.shields.io/badge/SQL-PostgreSQL-lightgrey" />
  <img alt="stack" src="https://img.shields.io/badge/Cloud-Azure-blue" />
  <img alt="stack" src="https://img.shields.io/badge/ETL-Streaming%20%7C%20Batch-green" />
</p>

---

## ðŸš€ About Me
I work with **data engineering and analytics**, designing pipelines, building structured datasets, and creating analytical models used for decision-making.  
For the last **4+ years**, Iâ€™ve also been developing a **personal predictive modeling system**, where I handle everything end-to-end: data ingestion, feature engineering, modeling, evaluation, automation, and monitoring.

I enjoy solving real problems with pragmatic, clean, and reliable data solutions.

---

## ðŸ”¬ What I Work On (Personally)
### ðŸ“ˆ Long-term Predictive Modeling System (4+ years)
A large personal project focused on applying statistical modeling, simulation, and ML techniques.  
Core components include:
- Automated data ingestion & cleaning  
- Feature engineering pipelines  
- Statistical models + ML ensembles  
- Bootstrap confidence, backtesting & metric monitoring  
- Continuous experiment cycles  
- Full reproducibility + versioning  

> This project reflects my technical depth, persistence, and ability to maintain a complex system long-term.

---

## ðŸ’¼ Professional Experience (Transfero)
I currently work as a **Data Analyst**, contributing to the companyâ€™s data infrastructure and analytics stack.

Key areas:
- **Resilient ETL pipelines** with Python (stream processing + idempotency patterns)  
- Data ingestion from multiple APIs and sources  
- **Serverless pipelines** with Azure Functions  
- Checkpointing, retries, and performance improvements  
- Data Lake â†’ Data Warehouse transformations (PostgreSQL)  
- Structured logging and secure workflow (Key Vault, Managed Identity)

My role strengthens my foundation in **data reliability, pipeline design, and cloud-based ingestion**.

---

## ðŸ›  Technical Skills
### **Languages & Tools**
- Python (Pandas, NumPy, Polars, SQLAlchemy)  
- SQL (PostgreSQL, query optimization, indexing)  
- Azure Functions, Blob Storage, Key Vault  
- Git, GitHub Actions  
- Docker  
- Jupyter, VSCode  
- Basic ML (scikit-learn, statsmodels, PyTorch for experimentation)

### **Engineering Practices**
- Idempotent ETL design  
- Retry & checkpoint strategies  
- Data quality checks  
- CI pipelines  
- Modular project structures  
- Reproducible experiments

---
